<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN">
<html lang="en">

<head>
    <title>Learning Tracker</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous" type=""></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
    <link href="https://fonts.googleapis.com/css?family=Frank+Ruhl+Libre|Roboto" rel="stylesheet">
    <link href="../../css/style.css" rel="stylesheet">
    <link href="../../css/normalize.css" rel="stylesheet" type="text/css" />
    <link href="../../css/toc.css" rel="stylesheet" type="text/css" />
    <link href="../../css/spoiler.css" rel="stylesheet" type="text/css" />
    <link href="../../js/comments/inlineDisqussions.css" rel="stylesheet" type="text/css" />
</head>

<body>

<div id="bump">
    <header class="site-header darken">
        <div class="wrap">
            <hgroup>
                <h1> Alexey Tochin </h1>
            </hgroup>
            <a class="nav menu" href="#"><span class='icons'>â˜°</span></a>
            <nav role="navigation">
                <ul>
                    <li><a href="../../index.html"> Home </a></li>
                    <li><a href="../../machine_learning.html"> ML </a></li>
                    <li><a href="../../other_topics.html"> Not ML </a></li>
                    <li><a href="../../about.html"> About </a></li>
                    <li><a href="../../references.html"> References </a></li>
                </ul>
            </nav>
        </div>
    </header>
</div>

<nav class="toc">
    <ul>
        <li> <a href="#Section-Notation"> Notation </a> </li>
        <li> <a href="#Section-Learning-rate-normalization"> Learning rate normalization </a> </li>
        <li> <a href="#Section-convergence-stability"> Convergence stability </a> </li>
        <li> <a href="#Section-Gradient-descent-with-momentum"> Gradient descent with momentum </a> </li>
        <li> <a href="#Section-Adam-gradient-descent-case"> Adam gradient descent case </a> </li>
        <li> <a href="#Section-Gradient-accumulation-technique"> Gradient accumulation technique </a> </li>
        <li> <a href="#Section-Conclusion"> Conclusion </a> </li>
    </ul>
    <svg class="toc-marker" width="200" height="200" xmlns="http://www.w3.org/2000/svg">
        <path
            stroke="#4c71b2"
            stroke-width="3"
            fill="transparent"
            stroke-dasharray="0, 0, 0, 1000"
            stroke-linecap="round"
            stroke-linejoin="round"
            transform="translate(-0.5, -0.5)"
        />
    </svg>
</nav>

<section class="article">
    <article class="wrap post">
            <header class="post-header">
                <hgroup>
                    <h1>Batch size vs Momentum</h1>
                </hgroup>
            </header>
            <p>
                Suppose we found an optimal learning rate and other hyperparameters for a gradient descent method and for a certain model.
                A typical next step is to train the model on a more power hardware with more GPU units and GPU RAM that supports bigger batch size. 
                How should we scale up the gradient descent hyperparameters and do not knock down the settings?
                We address this question and 
                some familiar ones in this article. 
                The main thesis:
                <em>batch size change is approximately equivalent to a certain shift of learning rate and momentum</em>.
            </p>
            <section id=Section-Notation>
                <h2> Notation </h2>
                <p>
                    Let
                    \( \boldsymbol\theta \in \mathbb{R}^D \)
                    be a trainable weight that is a vector in 
                    a
                    \( D \)-dimensional
                    space of the model trainable weights.
                    We denote by
                    \( \{\boldsymbol\theta_n\}_{n = 0,1,2,3\dotso} \)
                    a sequence of the weight updates during the training process.
                </p>
                <p>
                    Let also
                    \( L(x, y; \boldsymbol\theta) \)
                    be a loss function for a supervised model for weight
                    \( \boldsymbol\theta \),
                    and a single data sample with  
                    feature
                    \( x \),
                    and label
                    \( y \).
                    Let
                </p>
                \[\begin{aligned}
                	\mathbf{g}_i(\boldsymbol\theta) := \frac{\partial}{\partial \mathbf{\boldsymbol\theta}} L(x_i, y_i; \boldsymbol\theta)
                \end{aligned}\]
                <p>
                    be a loss gradient at point
                    \( \boldsymbol\theta \)
                    for sample
                    \( (x_i,y_i) \)
                    with an ordering number
                    \( i=0,1,2,\dotso \).
                </p>
                <p>
                    The samples
                    \( \{(x_i,y_i)\}_{i=0,1,2,\dotso} \)
                    are combined in batches. We restrict ourself with a fixed batch size
                    \( b=1,2,3,\dotso \)
                    case.
                    We define an integer valued function
                </p>
                \[\begin{aligned}
                	t:\quad \{0,1,2,\dotso\} \quad \rightarrow \quad \{0,1,2,\dotso\}
                \end{aligned}\]
                <p>
                    that maps the ordering number of each sample to the ordering number of the batch it belongs to.
                    For example, for
                    \( b=3 \)
                    we have
                </p>
                \[\begin{aligned}
                	t_0 = 0,~t_1 = 0,~t_2 = 0,~t_3 = 1,~t_4 = 1,~t_5 = 1,~t_6 = 2,~t_7 = 2,~t_8 = 2,~\dotso.
                \end{aligned}\]
            </section>
            <section id=Section-Learning-rate-normalization>
                <h2> Learning rate normalization </h2>
                <p>
                    We start from the simplest version of the stochastic gradient descent without momentum
                    parameterized by the learning rate
                    \( \lambda^{\text{cl}} > 0 \).
                    The weight update rule is
                </p>
                <div id="Fomula-classic-SGD-without-momentum">
                    <p class="formula_num"> (2.1) </p>
                    \[\begin{aligned}
                    	\boldsymbol\theta_{n-1}~ \rightarrow~ \boldsymbol\theta_n := 
                    	\boldsymbol\theta_{n-1} - \lambda^{\text{cl}} \, b^{-1} \sum_{i:~ t(i) = n} \mathbf{g}_i(\boldsymbol\theta_n),
                    \end{aligned}\]
                </div>
                <p>
                    where
                    \( \sum_{i:~ t(i) = n}~\dotso \)
                    is a sum over the minibatch
                    \( n \).
                    The upper index
                    <q>\( ~^{\text{cl}} \)</q>
                    is for
                    <q>classically normalized</q>
                    hyperparameter normalization that we are going to reconsider.
                </p>
                <p>
                    Suppose the batch size
                    \( b \)
                    has been changed.
                    Our purpose is to scale up
                    \( \lambda^{\text{cl}} \)
                    such that the shift of 
                    weight sequence
                    \( \{\boldsymbol\theta_n\}_{n = 0,1,2,3,\dotso} \)
                    is minimized. 
                    More precisely, 
                    after the batch size change
                    \( b \rightarrow \tilde b \),
                    each of the weights
                    \( \tilde{\boldsymbol\theta}_{\tilde t(i)} \)
                    from new sequence
                    must be closed to the original ones
                    \( \boldsymbol\theta_{t(i)} \)
                    for each
                    \( i=0,1,2,\dotso \),
                    where
                    \( \tilde t \)
                    corresponds to new batch size
                    \( \tilde b \).
                    It is straightforward to observe that the reasonable learning rate update is
                </p>
                \[\begin{aligned}
                	\lambda^{\text{cl}} \rightarrow \tilde \lambda^{\text{cl}} =
                	\frac{b}{\tilde b}\,  \lambda^{\text{cl}}.
                \end{aligned}\]
                <p>
                    In other words, the coefficient
                    \( b^{-1} \)
                    in
                    <cite><a href="#Fomula-classic-SGD-without-momentum"> (2.1) </a></cite>
                    is not needed in order keep this behavior.
                </p>
                <p>
                    For example, if we increase the batch size by a factor
                    \(  k  \)
                    the total contribution 
                    from
                    \(  k  \)
                    sequential steps is replaced by a single step with
                    \(  k  \)
                    time bigger amount of samples.  
                    The only difference between these sums is that in the second case all gradient are calculated 
                    at the same weight, while for smaller batch, there are
                    \(  k  \)
                    different but closed wights corresponding to different sequential steps.
                </p>
                <p>
                    The mathematical motivation comes from the Taylor series approximation for small
                    \( \lambda^{\text{cl}} \)
                </p>
                \[\begin{aligned}
                	\mathbf{g}_i(\boldsymbol\theta_n) - \mathbf{g}_i(\boldsymbol\theta_{n-1}) = O\left(\left(\lambda^{\text{cl}} \right)^2\right),
                \end{aligned}\]
                <p>
                    because
                </p>
                \[\begin{aligned}
                	\boldsymbol\theta_n - \boldsymbol\theta_{n-1} = O(\lambda^{\text{cl}}).
                \end{aligned}\]
                <p>
                    In order to experimentally confirm this analytical reasoning we consider MNIST problem and 
                    a simple convolution neural network. 
                    We do not use advanced techniques such as batch normalization to avoid unnecessary additional effects.
                </p>
                <div class="wrap-collabsible">
                    <input id="<label.Label object at 0x7f1c24b2fad0>" class="toggle" type="checkbox">
                    <label for="<label.Label object at 0x7f1c24b2fad0>" class="lbl-toggle"> Model details </label>
                    <div class="collapsible-content"> <div class="content-inner">
                        <code id="python_code"><br>
                            model&#8194;=&#8194;tf.keras.Sequential([<br>
                            &#8194;&#8194;&#8194;&#8194;tf.keras.layers.Reshape(input_shape=(28,&#8194;28,&#8194;1),&#8194;target_shape=(28,&#8194;28,&#8194;1)),<br>
                            &#8194;&#8194;&#8194;&#8194;tf.keras.layers.Conv2D(kernel_size=3,&#8194;filters=12,&#8194;padding='same',&#8194;activation='relu'),<br>
                            &#8194;&#8194;&#8194;&#8194;tf.keras.layers.Conv2D(kernel_size=6,&#8194;filters=24,&#8194;padding='same',&#8194;strides=2,&#8194;activation='relu'),<br>
                            &#8194;&#8194;&#8194;&#8194;tf.keras.layers.Conv2D(kernel_size=6,&#8194;filters=32,&#8194;padding='same',&#8194;strides=2,&#8194;activation='relu'),<br>
                            &#8194;&#8194;&#8194;&#8194;tf.keras.layers.Flatten(),<br>
                            &#8194;&#8194;&#8194;&#8194;tf.keras.layers.Dense(units=256,&#8194;activation='relu'),<br>
                            &#8194;&#8194;&#8194;&#8194;tf.keras.layers.Dropout(0.3),<br>
                            &#8194;&#8194;&#8194;&#8194;tf.keras.layers.Dense(units=10,&#8194;activation=None)<br>
                            ])
                        </code>
                        <p>
                            The loss function is the standard classification cross-entropy.
                        </p>
                    </div> </div>
                </div>
                <p>
                    It is practically complicated to compare the weight update sequences directly.
                    Instead, we consider the entire training progress and the mean loss convergence rate.
                    Considered model does not suffer from essential overfit. 
                    Thus we present for simplicity only the train dataset mean values below.
                </p>
                <p>
                    We verify experimentally that the convergence rate does not depends on
                    the batch size provided
                    \( \lambda \)
                    is fixed, see fig. below
                    <cite><a href="#Fig-batch-size-and-lambda"> 1</a></cite>.
                </p>
                <figure id="Fig-batch-size-and-lambda">
                    <img src="images/batch_size_and_lambda.png">
                    <figcaption>
                        Train progress evaluated on MNIST train set 
                        with respect to the number of elapsed samples (do not mix with the update steps). 
                        We can observe that the convergence rate does not depend on the batch size provided
                        \( \lambda \)
                        is fixed. On the other hand, the convergence rate, 
                        calculated in terms of the calculation cost, reduces dramatically for small
                        \( \lambda \).
                    </figcaption>
                </figure>
                <p>
                    The speculations above motivates us to normalize the learning rate
                    \( \lambda \)
                    as follows
                </p>
                \[\begin{aligned}
                	\boldsymbol\theta_n = 
                	\boldsymbol\theta_{n-1} - \lambda \sum_{i:~ t(i) = n} \mathbf{g}_i(\boldsymbol\theta_n),
                \end{aligned}\]
                <p>
                    thereby,
                </p>
                <div id="Formula-l^lc-=-b-l">
                    <p class="formula_num"> (2.2) </p>
                    \[\begin{aligned}
                    	\lambda^{\text{cl}} = b \, \lambda.
                    \end{aligned}\]
                </div>
                <p>
                    If the batch size
                    \( b \)
                    equals to the entire dataset size
                    \( N \)
                    or it is of the same order of magnitude
                    \( b \lesssim N \)
                    the update on each learning step moves the weight closer the minimum.
                    But this is not the case in the mini-batch strategy when
                    \( b \ll N \),
                    because each mini-batch represents very poor statistics of the entire dataset.
                    The mini-batch learning works due to the statistics averaging effect after numerous of learning step updates thanks to an aggregation of data collected from huge number of mini-batches. 
                    This motivates the following:
                    <em>In the mini-batch learning, it is probably more reasonable to control the total number of samples used for the updates then the actual number of the updates.</em>
                </p>
                <p>
                    We have to mention here that similarity of the training weight update sequences at early and intermediate  training stages does not imply that the quality of the achieved minima are similar. 
                    However, our approach also agrees with the empirical observations of
                    <a href="https://arxiv.org/abs/1710.06451" title="https://arxiv.org/abs/1710.06451">
                    Samuel L. Smith and Quoc V. Le</a>.
                    It is reported there that
                    <q>...optimum batch size is proportional to the learning rate...</q>.
                    In other words, there exist
                    a batch size independent
                    \( \lambda \)
                    that minimizes the final test set loss.
                </p>
                <p>
                    As it is mentioned in cited paper,
                    large batch size contributes to better train set minimum.
                    On the other hand, small batch size provides additional gradient fluctuation improving generalization ability.
                    The optimal value of
                    \( \lambda \)
                    corresponds to a reasonable trade off
                    between these two effects.
                </p>
                <p>
                    Notice, however, that the speculations of this section are valid only if the 
                    convergence is stable. It becomes unstable if the learning rate or batch size are too high. 
                    We study this phenomenon briefly in the next section.
                </p>
            </section>
            <section id=Section-convergence-stability>
                <h2> Convergence stability </h2>
                <p>
                    In this section, we investigate experimentally the stability of the neural network training process. It is widely known that the gradient descent usually becomes unstable when the learning rate exceeds some value. 
                    This is straightforward for a classical optimization problem, that is, 
                    the batch equals to the entire dataset. 
                    However, the instability is more complex for the mini-batch training.
                </p>
                <p>
                    The results for the introduced above MNIST problem are presented in fig.
                    <cite><a href="#Fig-convergence-stability"> 2</a></cite>.
                </p>
                <figure id="Fig-convergence-stability">
                    <img src="images/convergence_stability.png">
                    <figcaption>
                        Each dot corresponds to a training experiment and the color denotes the loss function value on the training set after 10000 weight updates. If the mean loss value is higher then
                        \( \log 10 \),
                        which corresponds to fully random classification for the 10-classes MNIST problem,
                        we replace it by
                        \( \log 10 \).
                        Both images are based on the same data but different learning rate normalization, 
                        see
                        <cite><a href="#Formula-l^lc-=-b-l"> (2.2) </a></cite>.
                        We can observe that the boundary between convergence and divergence modes is consists 
                        of two approximately line segments. Both segments corresponds to some constant values of
                        \( \lambda \)
                        and
                        \( \lambda^{\text{cl}} \).
                    </figcaption>
                </figure>
                <p>
                    Our interpretation is as follows.
                    There are two factors restricting the stability. 
                    The first one comes from the classical optimization theory.
                    It is defined by the Hessian of the mean loss function
                    \(  b^{-1} \sum_{i}{L(x_i,y_i)}  \)
                    at the minimum.
                    This is why we have the restriction
                    \( \lambda^{\text{cl}} \lesssim 10^{-0.5} \).
                </p>
                <p>
                    The second restriction
                    \( \lambda \lesssim 10^{-1.5} \)
                    is less trivial. It occurs when the training process is in the mini-batch mode. It means that the batches are not statistically representative 
                    and the training progress is made thanks to the numerous step accumulation. 
                    Thereby, the stability of the training is defined by
                    \( \lambda \),
                    which is a coefficient near the full sum of the gradients.
                </p>
                <p>
                    We do no go into more details of the stability question. 
                    But we have to mention here that the statements from the previous section is valid only if the training process is in the green zone of fig.
                    <cite><a href="#Fig-convergence-stability"> 2</a></cite>.
                </p>
            </section>
            <section id=Section-Gradient-descent-with-momentum>
                <h2> Gradient descent with momentum </h2>
                <p>
                    Our next step is to study the same thing but equip the gradient descent with the momentum. 
                    The most widely used parametrization for this approach is as follows
                </p>
                <div id="Fomula-classic-SGD-with-momentum">
                    <p class="formula_num"> (4.1) </p>
                    \[\begin{aligned}
                    	\begin{aligned}
                    		\mathbf{v}^{\text{cl}}_n 
                    			&= m^{\text{cl}}\, \mathbf{v}^{\text{cl}}_{n-1}
                    			- \lambda^{\text{cl}}\, b^{-1} \sum_{i:~ t(i) = n} \mathbf{g}_i(\boldsymbol\theta_n) \\
                    		\mathbf{\boldsymbol\theta}_n &= 
                    			\boldsymbol\theta_{n-1} + \mathbf{v}^{\text{cl}}_n
                    	\end{aligned},\quad
                    	0 \leq m^{\text{cl}} <1, \quad
                    	\lambda^{\text{cl}} > 0  
                    \end{aligned}\]
                </div>
                <p>
                    There is a widely mentioned 
                    physical interpretation
                    for the parameter
                    \( \mathbf{v}^{\text{cl}} \).
                    Indeed, if we consider
                    \( n \)
                    as a discrete
                    <q>time</q>
                    then
                    \( \mathbf{v}^{\text{cl}} \)
                    can be understood as
                    <q>velocity</q>
                    in the space of weights. 
                    This way the batch averaged gradient plays the role of
                    <q>acceleration</q>.
                    But we focus on a different aspect of the momentum trick here.
                </p>
                <p>
                    The exponential moving average for the sequence
                    \( \{g_n\}_{n=0,1,2,\dotso} \)
                    is a sequence
                    \( \{\mu_n\}_{n=0,1,2,\dotso} \)
                    defined by the recurrent relation
                </p>
                \[\begin{aligned}
                	\mu_n = m\, \mu_{n-1} + (1 - m)\, g_n, \quad 0 \leq m < 1
                \end{aligned}\]
                <p>
                    with some initial condition for
                    \( \mu_0 \)
                    that we are not interested.
                    Integrating it up we have
                </p>
                \[\begin{aligned}
                	\mu_n = (1 - m)\sum_{k=0,1,2,\dotso} \, m^k\, g_{n-k}.
                \end{aligned}\]
                <p>
                    Thereby, the momentum trick is nothing else then the smoothing of gradients calculated 
                    on the previous steps. 
                    Equivalently, 
                    instead of taking into account
                    only the gradients calculated at the current weight
                    \( \boldsymbol\theta_n \)
                    at step
                    \( n \),
                    we sum up gradients obtained at all previous weights
                    \( \boldsymbol\theta_{n-1} \),
                    \( \boldsymbol\theta_{n-2} \),
                    \( \boldsymbol\theta_{n-3} \),
                    ...
                    with decreasing weights coefficients
                    \( (1-m)m \),
                    \( (1-m)m^2 \),
                    \( (1-m)m^3 \),
                    ...
                    correspondingly.
                    Notice that the sum of the weights is normalized as follows
                </p>
                \[\begin{aligned}
                	(1-m)\sum_{k=0,1,2,\dotso} \, m^k = 1, \quad 0 \leq m < 1.
                \end{aligned}\]
                <p>
                    The parameter
                    \( m \)
                    is responsible for the rate of the weight decreasing. 
                    We wish to make this rate fixed in terms of sample counting but not update steps as usual.
                    Namely, the weight coefficient with which each single sample gradient
                    \( g_i \)
                    contributes should be modified minimally with the transform from one batch size to another. This implies
                </p>
                <div id="Formula-m^1/b-=-m^1/b">
                    <p class="formula_num"> (4.5) </p>
                    \[\begin{aligned}
                    	\sqrt[b]{m} = \sqrt[\tilde b]{\tilde m}.
                    \end{aligned}\]
                </div>
                <p>
                    In other words, after he batch size change the momentum must be rescaled as
                </p>
                <div id="Formula-m->m^b/b">
                    <p class="formula_num"> (4.6) </p>
                    \[\begin{aligned}
                    	m \rightarrow \tilde m = m^{\frac{\tilde b}{b}}
                    \end{aligned}\]
                </div>
                <p>
                    in order to keep the same manner for the gradient smoothing over samples. 
                    Fig
                    <cite><a href="#Fig-momentum-weigth-coefficients"> 3</a></cite>
                    can probably illustrates this idea better then words and formulas.
                </p>
                <figure id="Fig-momentum-weigth-coefficients">
                    <img src="images/momentum_weigth_coefficients.png">
                    <figcaption>
                        Weight coefficients with which each sample gradient
                        \( g_i \)
                        contributes to a train step with 
                        condition
                        <cite><a href="#Formula-m^1/b-=-m^1/b"> (4.5) </a></cite>.
                        The batch sizes are
                        \( 4 \)
                        and
                        \( 16 \),
                        and
                        \( \sqrt[b]{m} = \sqrt[\tilde b]{\tilde m} = 0.03 \).
                        The x-axis corresponds to the reverse sample counting.
                        The weight coefficients values are visually combined into blocks of sizes
                        \( b=4 \)
                        or
                        \( \tilde b = 16 \)
                        depending on the batch size.
                        The image illustrates that, in spite of different batch size, the momentum can be tuned 
                        such that the weight coefficients are closed by their value.
                    </figcaption>
                </figure>
                <p>
                    <em>The ultimate goal of our gradient descent hyperparameter reparametrization is to split out the technical part related to data parallelism and real smoothing between gradients during the minima search.</em>
                    This motivates an alternative parametrization for
                    <cite><a href="#Fomula-classic-SGD-with-momentum"> (4.1) </a></cite>
                    as follows.
                </p>
                <div id="Formula-SGD-with-momentum-reparametrized">
                    <p class="formula_num"> (4.7) </p>
                    \[\begin{aligned}
                    	\begin{aligned}
                    		\mathbf{v}_n &= e^{-\frac{b}{\beta}}\, \mathbf{v}_{n-1}
                    		 	+ (1 - e^{-\frac{b}{\beta}})\, \sum_{i:\, t(i) = n} \mathbf{g}_i(\boldsymbol\theta_n) \\
                    		\boldsymbol\theta_n &= 
                    			\boldsymbol\theta_{n-1} - \lambda\, \mathbf{v}_n
                    	\end{aligned},\quad
                    	\beta \geq 0, \quad
                    	\lambda > 0.
                    \end{aligned}\]
                </div>
                <p>
                    The case
                    \( \beta = 0 \)
                    corresponds to
                    \( m=0 \).
                    The bigger value
                    \( \beta \)
                    the stronger smoothing.
                </p>
                <p>
                    The advantage of the proposed approach is that we do not have to change 
                    the fine tuned hyperparameters
                    \( \lambda \)
                    and
                    \( \beta \)
                    while moving to a different hardware.
                    It is enough to just set the batch size
                    \( b \)
                    as big as possible (or until we face with the convergence stability restrictions 
                    form section
                    <cite><a href="#Section-convergence-stability"> 3</a></cite>
                    ). 
                    The experimental confirmation is presented in fig.
                    <cite><a href="#Fig-batch-size-and-momentum"> 4</a></cite>.
                </p>
                <figure id="Fig-batch-size-and-momentum">
                    <img src="images/batch_size_and_momentum.png">
                    <figcaption>
                        Training progress on the train MNIST dataset with various values of the batch size
                        \( b \)
                        and
                        \( \beta \)
                        from
                        <cite><a href="#Formula-SGD-with-momentum-reparametrized"> (4.7) </a></cite>.
                        For each pair of
                        \( b \)
                        and
                        \( \beta \)
                        values we sampled 3 experiments that are demonstrated by 3 corresponding curves of the same color and style.
                        It can be observed that batch size change has no visible effect on the loss decay rate
                        with respect to the total number of sample count.
                        It is also remarkable that the smaller values for 
                        momentum parameter
                        \( \beta \)
                        provides faster convergence in our case. This may be different for other problems.
                    </figcaption>
                </figure>
                <p>
                    Notice, however, that for
                    \( b \gtrsim \beta \)
                    the momentum smoothing is weak with respect to
                    <q>batch</q>
                    smoothing and the approximation illustrated in fig.
                    <cite><a href="#Fig-momentum-weigth-coefficients"> 3</a></cite>
                    is not longer valid.
                </p>
                <p>
                    The relations between proposed and classic hyperparameter normalization are
                    \[\begin{aligned}
                    m^{\text{cl}} &= e^{-\frac{b}{\beta}} \\ 
                    \lambda^{\text{cl}} &= \left(1 - e^{-\frac{b}{\beta}} \right) \lambda\, b^{-1}   
                    \end{aligned}\]
                </p>
            </section>
            <section id=Section-Adam-gradient-descent-case>
                <h2> Adam gradient descent case </h2>
                <p>
                    For other gradient descent methods our approach is similar. 
                    For example, Adam gradient descent in its classic parametrization in our notations is
                </p>
                <div id="Fomula-Adam-SGD">
                    <p class="formula_num"> (5.1) </p>
                    \[\begin{aligned}
                    	\begin{aligned}
                    		\mathbf{v}^{\text{cl}}_n 
                    			&= \beta_1^{\text{cl}}\, \mathbf{v}^{\text{cl}}_{n-1}
                    			+ (1 - \beta_1^{\text{cl}})\, b^{-1} \sum_{i:~ t(i) = n} \mathbf{g}_i(\boldsymbol\theta_n), \\
                    		\mathbf{w}^{\text{cl}}_n 
                    			&= \beta_2^{\text{cl}}\, \mathbf{w}^{\text{cl}}_{n-1}
                    			+ (1 - \beta_2^{\text{cl}})\, 
                    				\left[
                    					b^{-1} \sum_{i:~ t(i) = n} \mathbf{g}_i(\boldsymbol\theta_n) 
                    				\right]^2, \\
                    		\boldsymbol\theta_n &= 
                    			\boldsymbol\theta_{n-1} - \alpha^{\text{cl}} \frac
                    				{\mathbf{m}^{\text{cl}}_n}
                    				{\sqrt{\mathbf{v}^{\text{cl}}_n} + \varepsilon^{\text{cl}}}, \\
                    		&0 \leq \beta_1^{\text{cl}} <1, \quad
                    		0 \leq \beta_2^{\text{cl}} <1, \quad
                    		\alpha^{\text{cl}} > 0, \quad
                    		\varepsilon^{\text{cl}} > 0,
                    	\end{aligned}
                    \end{aligned}\]
                </div>
                <p>
                    where the square and square root operations of a vector are elementwise. 
                    We proposed the normalization
                    \[\begin{aligned}
                    \mathbf{v}_n 
                    &= e^{-\frac{b}{\beta_1}}\, \mathbf{v}_{n-1}
                    + (1 - e^{-\frac{b}{\beta_1}})\, \sum_{i:~ t(i) = n} \mathbf{g}_i(w_n), \\
                    \mathbf{w}_n 
                    &= e^{-\frac{b}{\beta_2}}\, \mathbf{w}_{n-1}
                    + (1 - e^{-\frac{b}{\beta_2}})\, 
                    \sum_{i:~ t(i) = n} \mathbf{g}_i(w_n)^2, \\
                    \boldsymbol\theta_n &= 
                    \boldsymbol\theta_{n-1} - \alpha \frac
                    {\mathbf{v}_n}
                    {\sqrt{\mathbf{w}_n} + b\, \varepsilon}, \\
                    & 0 \leq \beta_1, \quad
                    0 \leq \beta_2, \quad
                    \alpha > 0, \quad 
                    \varepsilon > 0,
                    \end{aligned}\]
                    where we have to step a side a bit from the original Adam algorithm and replace the square of the gradient sum by the sum of gradient squares along the batch. It can be empirically check that the typical values of
                    \( (\sum g)^2 \)
                    and
                    \( \sum g^2 \)
                    are of the same order of magnitude. 
                    At least for the considered MNIST model typical values of
                    \( \sum g^2 \)
                    are approximately two times smaller then
                    \( (\sum g)^2 \)
                    or equal.
                </p>
                <p>
                    Ignoring the last remark, the relations between the proposed and the classic normalizations are
                </p>
                <div id="Formula-Adam-hyparameter-relation">
                    <p class="formula_num"> (5.2) </p>
                    \[\begin{aligned}
                    	\begin{aligned}
                    		\beta_1^{\text{cl}} &= e^{-\frac{b}{\beta_1}}, \\ 
                    		\beta_2^{\text{cl}} &= e^{-\frac{b}{\beta_2}}, \\ 
                    		\alpha^{\text{cl}} &= \alpha, \\
                    		\varepsilon^{\text{cl}} &= b\,\varepsilon.
                    	\end{aligned}
                    \end{aligned}\]
                </div>
            </section>
            <section id=Section-Gradient-accumulation-technique>
                <h2> Gradient accumulation technique </h2>
                <p>
                    The relation between momentum and batch size discussed above is quite straightforward. 
                    However, there is a quite widely used
                    <a href="https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa" title="https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa">
                    technique</a>
                    called
                    <em>gradient accumulation</em>.
                    According to this approach, we sum up gradients from from several batches
                    obtained for the same weight value. Thereby, the actual batch size can be increased infinitely
                    and GPU RAM is not longer a restriction.
                </p>
                <p>
                    As we see in section
                    <cite><a href="#Section-convergence-stability"> 3</a></cite>,
                    for zero momentum 
                    higher batch size with fixed
                    \( \lambda \)
                    can only corrupt the training process.
                    For the momentum training
                    \( \beta \gtrsim b \)
                    the batch size change is undistinguishable from
                    \( \beta \)
                    shift as we saw in section
                    <cite><a href="#Section-Gradient-descent-with-momentum"> 4</a></cite>.
                    Of course the gradient accumulation trick can provide a better result in practice
                    but it looks redundant.
                </p>
            </section>
            <section id=Section-Conclusion>
                <h2> Conclusion </h2>
                <p>
                    We suggest a strategy to fine tune the gradient descent hyperparameters as follows.
                    Use hyperparameter normalization as it is described above, for example
                    <cite><a href="#Formula-Adam-hyparameter-relation"> (5.2) </a></cite>.
                    As the result of this, we can control the convergence rate while batch size in changing.
                    It may happen, however, following the increase of the batch size the convergence stability may be lost.
                    This case we can either reduce
                    \( \lambda \)
                    to restore the convergence or refuse from the batch size changing.
                    The second option has an advantage of faster convergence with respect to total calculation cost.
                </p>
                <p>
                    Remark that we did not consider batch normalization technique here. 
                    This requires additional study because for different batch sizes we have different gradient fluctuation intensity that impacts final minimum generalization ability. See, for example,
                    <a href="https://arxiv.org/pdf/1802.06455.pdf" title="https://arxiv.org/pdf/1802.06455.pdf">
                    Mattias Teye, Hossein Azizpour, Kevin Smith</a>.
                </p>
            </section>
            <hr>
            <p>
                <em>
                    If you have any questions, remarks, or other feedback please use
                    <a href="https://t.me/Alexey_Tochin"> telegram </a>,
                    <a href="mailto:Alexey.Tochin@gmail.com"> e-mail</a>,
                    or
                    <a href="https://github.com/alexeytochin/alexeytochin.github.io/discussions/8"> this github forum </a>
                    for open discussions.
                </em>
            </p>
        <!-- Comment section -->
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
        <script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>
        <script src="../../js/comments/inlineDisqussions.js"></script>
        <script src="../../js/comments/disqus.js"></script>
        <div id="disqus_thread"></div>
    </article>
</section>

<script src="../../js/toc.js"></script>
<script src="../../js/script.js"></script>

</body>
</html>